{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd73764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 0: get all elements in page 1\n",
    "url = \"https://cnp.hk/eng/tranp.php?p=&0=&e=&y=&d=ALL&b=&t=L&s=\"\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "driver.get(url)\n",
    "parsed_html = soup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1969cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: get all col name in page 1 table\n",
    "filter_col_name = \"hidden-xs table table-hover\"\n",
    "layer = parsed_html.find(class_=filter_col_name)\n",
    "\n",
    "col_name = []\n",
    "for i in layer.tr:\n",
    "    col_name.append(i.text) if i.text != \"\\n\" else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: get all classes of haunted_sep (for flat deal info.) in page 1\n",
    "filter1 = \"haunted_sep\"\n",
    "layer1 = layer.find_all(class_=filter1)\n",
    "\n",
    "pre_df = []\n",
    "till_when = \"2019-05-02\"\n",
    "breaker = False\n",
    "while True:\n",
    "    for haunted_sep in layer1:\n",
    "        row = []\n",
    "        for text_cen in haunted_sep:\n",
    "            t = text_cen.text\n",
    "            if till_when in t:      # stop the loop once arrived the date range (oldest)\n",
    "                breaker = True      # *use \"breaker = True\" to break multi loops*\n",
    "                break               # break the innermost loop (which is the 2nd for-loop)\n",
    "            else:\n",
    "                row.append(t) if t != \"\\n\" else None\n",
    "        if breaker:\n",
    "            break                   # break the 1st for-loop\n",
    "        else:\n",
    "            pre_df.append(row)      # gather rows into pre_df\n",
    "    if breaker:\n",
    "        break                       # break the outermost loop (which is a while-loop)\n",
    "    n = driver.find_element(By.LINK_TEXT, \"Next\")\n",
    "    if n:\n",
    "        n.click()                   # go to the next page after scrapped the current page\n",
    "    else:\n",
    "        break       # stop scrapping when there's no more \"next\" button\n",
    "    time.sleep(0.3)  # give buffer time to load the page\n",
    "    driver.get(driver.current_url)  # get the current url after clicked \"next\" on the previous iteration\n",
    "    parsed_html = soup(driver.page_source, \"html.parser\") # re-do the html parsing & class searching in the current page\n",
    "    layer1 = parsed_html.find_all(class_=filter1)\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff317ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pre_df to dataframe and then to_csv\n",
    "df = pd.DataFrame(pre_df, columns=col_name)\n",
    "df.to_csv(\"cnp 3-yrs_web-scrap.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
